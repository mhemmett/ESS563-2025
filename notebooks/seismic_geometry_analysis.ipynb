{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79eef0",
   "metadata": {},
   "source": [
    "# Seismic Geometry Analysis\n",
    "\n",
    "This notebook processes seismic catalog data by:\n",
    "1. Loading catalog and station data\n",
    "2. Adding station coordinates to the catalog\n",
    "3. Calculating back azimuth and angle of incidence\n",
    "4. Filtering events based on angle of incidence criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7517f7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd030c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not import seismic geometry functions: No module named 'seismic_geometry'\n",
      "Will define basic geometry functions locally\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts directory to path to import custom modules\n",
    "scripts_path = os.path.join('..', 'scripts')\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "# Import seismic geometry functions\n",
    "try:\n",
    "    from seismic_geometry import calculate_back_azimuth, calculate_angle_of_incidence\n",
    "    print(\"Successfully imported seismic geometry functions\")\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import seismic geometry functions: {e}\")\n",
    "    print(\"Will define basic geometry functions locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963a160",
   "metadata": {},
   "source": [
    "## 2. Load Catalog and Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ed63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filtered catalog sample (100 earthquake events)\n",
    "catalog = pd.read_csv('../data/catalog_filtered_sample.csv')\n",
    "print(f\"Loaded catalog with {len(catalog)} rows and {len(catalog.columns)} columns\")\n",
    "print(f\"Catalog columns: {catalog.columns.tolist()}\")\n",
    "print(f\"Unique stations in catalog: {catalog['station'].unique()}\")\n",
    "\n",
    "# Load station information\n",
    "stations = pd.read_csv('../data/axial_seamount_stations.csv')\n",
    "print(f\"\\nLoaded station data with {len(stations)} rows and {len(stations.columns)} columns\")\n",
    "print(f\"Station columns: {stations.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows of each dataframe\n",
    "print(\"\\nFirst 5 rows of catalog:\")\n",
    "display(catalog.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of station data:\")\n",
    "display(stations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca8083",
   "metadata": {},
   "source": [
    "## 3. Merge Station Information with Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d49f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a station ID mapping - remove 'OO' prefix from catalog station names\n",
    "catalog['station_id'] = catalog['station'].str.replace('OO', '', regex=False)\n",
    "print(f\"Original station names: {catalog['station'].unique()}\")\n",
    "print(f\"Modified station IDs: {catalog['station_id'].unique()}\")\n",
    "\n",
    "# Check station names in the station dataframe\n",
    "print(f\"\\nStation names in station file: {stations.iloc[:, 0].unique()}\")\n",
    "\n",
    "# Merge catalog with station information based on station ID\n",
    "# Assuming the first column in stations contains the station ID\n",
    "station_id_column = stations.columns[0]\n",
    "stations_renamed = stations.rename(columns={station_id_column: 'station_id'})\n",
    "\n",
    "# Merge the dataframes\n",
    "catalog_with_stations = catalog.merge(\n",
    "    stations_renamed[['station_id', 'lat', 'lon', 'elev']], \n",
    "    on='station_id', \n",
    "    how='left',\n",
    "    suffixes=('', '_station')\n",
    ")\n",
    "\n",
    "# Rename station coordinates to avoid confusion with earthquake coordinates\n",
    "catalog_with_stations = catalog_with_stations.rename(columns={\n",
    "    'lat': 'station_lat',\n",
    "    'lon': 'station_lon', \n",
    "    'elev': 'station_elev'\n",
    "})\n",
    "\n",
    "print(f\"\\nMerged catalog shape: {catalog_with_stations.shape}\")\n",
    "print(f\"Number of rows with station coordinates: {catalog_with_stations['station_lat'].notna().sum()}\")\n",
    "\n",
    "# Display the merged dataframe\n",
    "print(\"\\nFirst 5 rows of merged catalog with station coordinates:\")\n",
    "display(catalog_with_stations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5186656",
   "metadata": {},
   "source": [
    "## 4. Calculate Seismic Geometry Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4600db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate geometry parameters for each row using imported functions\n",
    "print(\"Calculating back azimuth and angle of incidence...\")\n",
    "\n",
    "# Filter out rows without station coordinates\n",
    "valid_rows = catalog_with_stations['station_lat'].notna()\n",
    "print(f\"Rows with valid station coordinates: {valid_rows.sum()}\")\n",
    "\n",
    "# Calculate back azimuth and angle of incidence\n",
    "catalog_with_stations['back_azimuth'] = np.nan\n",
    "catalog_with_stations['angle_of_incidence'] = np.nan\n",
    "\n",
    "for idx, row in catalog_with_stations[valid_rows].iterrows():\n",
    "    try:\n",
    "        # Calculate back azimuth (earthquake to station)\n",
    "        baz = calculate_back_azimuth(\n",
    "            row['lat'], row['lon'], \n",
    "            row['station_lat'], row['station_lon']\n",
    "        )\n",
    "        catalog_with_stations.loc[idx, 'back_azimuth'] = baz\n",
    "        \n",
    "        # Calculate angle of incidence\n",
    "        aoi = calculate_angle_of_incidence(\n",
    "            row['lat'], row['lon'], row['depth'],\n",
    "            row['station_lat'], row['station_lon'], row['station_elev']\n",
    "        )\n",
    "        catalog_with_stations.loc[idx, 'angle_of_incidence'] = aoi\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating geometry for row {idx}: {e}\")\n",
    "\n",
    "print(f\"Successfully calculated geometry for {catalog_with_stations['back_azimuth'].notna().sum()} rows\")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"\\nBack azimuth range: {catalog_with_stations['back_azimuth'].min():.1f} to {catalog_with_stations['back_azimuth'].max():.1f} degrees\")\n",
    "print(f\"Angle of incidence range: {catalog_with_stations['angle_of_incidence'].min():.1f} to {catalog_with_stations['angle_of_incidence'].max():.1f} degrees\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample of calculated geometry parameters:\")\n",
    "display(catalog_with_stations[['station', 'lat', 'lon', 'depth', 'station_lat', 'station_lon', 'back_azimuth', 'angle_of_incidence']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e499729",
   "metadata": {},
   "source": [
    "## 5. Filter by Angle of Incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ccee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter earthquakes with angle of incidence <= 30 degrees\n",
    "print(f\"Original dataset shape: {catalog_with_stations.shape}\")\n",
    "print(f\"Rows with valid angle of incidence: {catalog_with_stations['angle_of_incidence'].notna().sum()}\")\n",
    "\n",
    "# Create filter for angle of incidence <= 30 degrees\n",
    "aoi_filter = catalog_with_stations['angle_of_incidence'] <= 30.0\n",
    "valid_aoi = catalog_with_stations['angle_of_incidence'].notna()\n",
    "\n",
    "# Apply both filters (valid and <= 30 degrees)\n",
    "final_filter = valid_aoi & aoi_filter\n",
    "\n",
    "catalog_final = catalog_with_stations[final_filter].copy()\n",
    "\n",
    "print(f\"\\nFiltered dataset shape: {catalog_final.shape}\")\n",
    "print(f\"Rows removed: {catalog_with_stations.shape[0] - catalog_final.shape[0]}\")\n",
    "\n",
    "# Statistics about filtering\n",
    "print(f\"\\nAngle of incidence statistics before filtering:\")\n",
    "print(f\"  Mean: {catalog_with_stations['angle_of_incidence'].mean():.1f} degrees\")\n",
    "print(f\"  Min: {catalog_with_stations['angle_of_incidence'].min():.1f} degrees\") \n",
    "print(f\"  Max: {catalog_with_stations['angle_of_incidence'].max():.1f} degrees\")\n",
    "print(f\"  Rows > 30 degrees: {(catalog_with_stations['angle_of_incidence'] > 30).sum()}\")\n",
    "\n",
    "print(f\"\\nAngle of incidence statistics after filtering:\")\n",
    "print(f\"  Mean: {catalog_final['angle_of_incidence'].mean():.1f} degrees\")\n",
    "print(f\"  Min: {catalog_final['angle_of_incidence'].min():.1f} degrees\")\n",
    "print(f\"  Max: {catalog_final['angle_of_incidence'].max():.1f} degrees\")\n",
    "\n",
    "# Display distribution by station\n",
    "print(f\"\\nEvent distribution by station after filtering:\")\n",
    "print(catalog_final['station'].value_counts().sort_index())\n",
    "\n",
    "# Show final dataframe\n",
    "print(f\"\\nFinal filtered catalog:\")\n",
    "display(catalog_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb29bfd",
   "metadata": {},
   "source": [
    "## 6. Save Filtered Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final filtered catalog with geometry parameters\n",
    "output_file = '../data/catalog_with_geometry_filtered.csv'\n",
    "catalog_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved filtered catalog with geometry parameters to: {output_file}\")\n",
    "print(f\"Final dataset contains {len(catalog_final)} rows with {len(catalog_final.columns)} columns\")\n",
    "\n",
    "print(f\"\\nFinal column list:\")\n",
    "for i, col in enumerate(catalog_final.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(f\"  Total events: {len(catalog_final)}\")\n",
    "print(f\"  Unique earthquake IDs: {catalog_final['id'].nunique()}\")\n",
    "print(f\"  Stations represented: {catalog_final['station'].nunique()}\")\n",
    "print(f\"  Year range: {catalog_final['year'].min()} - {catalog_final['year'].max()}\")\n",
    "print(f\"  Magnitude range: {catalog_final['mag'].min():.1f} - {catalog_final['mag'].max():.1f}\")\n",
    "print(f\"  Depth range: {catalog_final['depth'].min():.1f} - {catalog_final['depth'].max():.1f} km\")\n",
    "print(f\"  Back azimuth range: {catalog_final['back_azimuth'].min():.1f} - {catalog_final['back_azimuth'].max():.1f} degrees\")\n",
    "print(f\"  Angle of incidence range: {catalog_final['angle_of_incidence'].min():.1f} - {catalog_final['angle_of_incidence'].max():.1f} degrees\")\n",
    "\n",
    "print(f\"\\nDataset is ready for shear-wave splitting analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
